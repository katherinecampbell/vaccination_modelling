{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kec52\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.collocations import *\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import re\n",
    "from nltk.text import Text\n",
    "import string \n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import modules & set up logging\n",
    "import gensim, logging\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_txt(file):\n",
    "    with open(file, encoding=\"utf8\") as f:\n",
    "        raw = f.read() \n",
    "        sentenceSplit = nltk.sent_tokenize(raw)\n",
    "        return sentenceSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lemmatize for verbs\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        lemma = word\n",
    "    return lemma\n",
    "\n",
    "#lemmatize for nouns\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "def no_stop(txt):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    text = []\n",
    "    for sentence in txt:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        print(sentence)\n",
    "        tokens = [token for token in tokens if len(token) > 2]\n",
    "        tokens = [token for token in tokens if token not in stop]\n",
    "        tokens = [get_lemma2(token) for token in tokens]\n",
    "        new_sentence = \"\".join([\" \"+i if i not in string.punctuation else i for i in tokens]).strip()\n",
    "        text.append(new_sentence)\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stop word from txt files to lists\n",
    "with open ('C:\\\\Users\\\\kec52\\\\BL_research\\\\BL_research_files\\\\py_src\\\\latin_stop.txt', 'r') as latin:\n",
    "    latin_stop = [word.strip(\",.\") for line in latin for word in line.lower().split()]\n",
    "    latin_stop.append('ab')\n",
    "\n",
    "with open ('C:\\\\Users\\\\kec52\\\\BL_research\\\\BL_research_files\\\\py_src\\\\german_stop.txt', 'r') as german:\n",
    "    german_stop = [word.strip(\",.\") for line in german for word in line.lower().split()]\n",
    "    german_stop.append('aber')\n",
    "    \n",
    "with open ('C:\\\\Users\\\\kec52\\\\BL_research\\\\BL_research_files\\\\py_src\\\\french_stop.txt', 'r') as french:\n",
    "    french_stop = [word.strip(\",.\") for line in french for word in line.lower().split()]\n",
    "    french_stop.append('alors')\n",
    "    \n",
    "with open ('C:\\\\Users\\\\kec52\\\\BL_research\\\\BL_research_files\\\\py_src\\\\spanish_stop.txt', 'r') as spanish:\n",
    "    spanish_stop = [word.strip(\",.\") for line in spanish for word in line.lower().split()]\n",
    "    spanish_stop.append('un')\n",
    "\n",
    "english_stop = list(STOPWORDS)\n",
    "\n",
    "extra_stop = ['tli', 'thk', 'http', 'org', 'ofthe', 'tha', 'tho', 'ther', 'there', 'der', 'dat', 'ain', 'tis', 'thee', 'thou', 'thy', 'waa']\n",
    "stop_words = english_stop + extra_stop + spanish_stop + german_stop + latin_stop + french_stop\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to clean and normalize all text\n",
    "def clean_it(txt):\n",
    "    allowed = {\" \", \"'\", \"-\", \"  \"}.union(string.ascii_lowercase, string.digits)\n",
    "    txt = list(map(str.lower,txt))\n",
    "    txt = [\"\".join([letter for letter in item if letter in allowed]) for item in txt]\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_txt(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    tokens = [get_lemma2(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find most freq words\n",
    "def freq(txt, num):\n",
    "    #find most frequent words\n",
    "    fd = nltk.FreqDist(txt)\n",
    "    print(fd.most_common(num))\n",
    "    fd.plot(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_it(dir,x):\n",
    "    for filename in os.listdir(dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            path = os.path.join(dir,filename)\n",
    "            raw = load_txt(path)\n",
    "            cleaned = clean_it(raw)\n",
    "            with open('C:\\\\Users\\\\kec52\\\\Documents\\\\BLtext_research\\\\BritishLib\\\\000'+x+'_clean_sent.txt', \"a\", encoding='utf-8') as f:\n",
    "                for s in cleaned:\n",
    "                    f.write(s)\n",
    "               \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "   # 'C:\\Users\\kec52\\Documents\\BLtext_research\\BritishLib\\0000_clean_sent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [' i ',' love ',' you ']\n",
    "for i in range (1,100):\n",
    "    with open('C:\\\\Users\\\\kec52\\\\Documents\\\\BLtext_research\\\\testing.txt', \"a\", encoding='utf-8') as f:\n",
    "        for word in x:\n",
    "            f.write(word)\n",
    "        f.write(str(i))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function Ngrams with 'word' as a member n=2 for bi n=3 for tri\n",
    "def filter_ngrams(text, word, n, freq):\n",
    "    word_filter = lambda *w: word not in w\n",
    "    ## Bigrams\n",
    "    if n==2:\n",
    "        finder = BigramCollocationFinder.from_words(text)\n",
    "        # only bigrams that appear freq+ times\n",
    "        finder.apply_freq_filter(freq)\n",
    "        # only bigrams that contain word\n",
    "        finder.apply_ngram_filter(word_filter)\n",
    "        # return the 10 n-grams with the highest PMI\n",
    "        print (finder.nbest(bigram_measures.likelihood_ratio, 10))\n",
    "    ## Trigrams\n",
    "    elif n==3:\n",
    "        finder = TrigramCollocationFinder.from_words(text)\n",
    "        # only trigrams that appear freq+ times\n",
    "        finder.apply_freq_filter(freq)\n",
    "        # only trigrams that contain word\n",
    "        finder.apply_ngram_filter(word_filter)\n",
    "        # return the 10 n-grams with the highest PMI\n",
    "        print (finder.nbest(trigram_measures.likelihood_ratio, 10))\n",
    "    else:\n",
    "        print('n must be value of 2 or 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram_finder (txt, n):\n",
    "    x = [item.split() for item in txt]\n",
    "    grams = [list(ngrams(i,n)) for i in x]\n",
    "    nngrams = [j for i in grams for j in i]\n",
    "    return nngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_it('C:\\\\Users\\\\kec52\\\\Documents\\\\BLtext_research\\\\BritishLib\\\\text_files\\\\0000','0')\n",
    "write_it('C:\\\\Users\\\\kec52\\\\Documents\\\\BLtext_research\\\\BritishLib\\\\text_files\\\\0001','1')\n",
    "write_it('C:\\\\Users\\\\kec52\\\\Documents\\\\BLtext_research\\\\BritishLib\\\\text_files\\\\0002','2')\n",
    "write_it('C:\\\\Users\\\\kec52\\\\Documents\\\\BLtext_research\\\\BritishLib\\\\text_files\\\\0003','3')\n",
    "write_it('C:\\\\Users\\\\kec52\\\\Documents\\\\BLtext_research\\\\BritishLib\\\\text_files\\\\0004','4')\n",
    "write_it('C:\\\\Users\\\\kec52\\\\Documents\\\\BLtext_research\\\\BritishLib\\\\text_files\\\\0005','5')\n",
    "write_it('C:\\\\Users\\\\kec52\\\\Documents\\\\BLtext_research\\\\BritishLib\\\\text_files\\\\0006','6')\n",
    "write_it('C:\\\\Users\\\\kec52\\\\Documents\\\\BLtext_research\\\\BritishLib\\\\text_files\\\\0007','7')\n",
    "write_it('C:\\\\Users\\\\kec52\\\\Documents\\\\BLtext_research\\\\BritishLib\\\\text_files\\\\0008','8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ProText1 = Text(nltk.corpus.gutenberg.words(\"C:\\\\Users\\\\kec52\\\\Documents\\\\BLtext_research\\\\BritishLib\\\\0000_clean_sent.txt\"))\n",
    "ProText2 = Text(nltk.corpus.gutenberg.words(\"C:\\\\Users\\\\kec52\\\\Documents\\\\BLtext_research\\\\BritishLib\\\\0001_clean_sent.txt\"))\n",
    "ProText3 = Text(nltk.corpus.gutenberg.words(\"C:\\\\Users\\\\kec52\\\\Documents\\\\BLtext_research\\\\BritishLib\\\\0002_clean_sent.txt\"))\n",
    "ProText4 = Text(nltk.corpus.gutenberg.words(\"C:\\\\Users\\\\kec52\\\\Documents\\\\BLtext_research\\\\BritishLib\\\\0003_clean_sent.txt\"))\n",
    "ProText5 = Text(nltk.corpus.gutenberg.words(\"C:\\\\Users\\\\kec52\\\\Documents\\\\BLtext_research\\\\BritishLib\\\\0004_clean_sent.txt\"))\n",
    "ProText6 = Text(nltk.corpus.gutenberg.words(\"C:\\\\Users\\\\kec52\\\\Documents\\\\BLtext_research\\\\BritishLib\\\\0005_clean_sent.txt\"))\n",
    "ProText7 = Text(nltk.corpus.gutenberg.words(\"C:\\\\Users\\\\kec52\\\\Documents\\\\BLtext_research\\\\BritishLib\\\\0006_clean_sent.txt\"))\n",
    "ProText8 =  Text(nltk.corpus.gutenberg.words(\"C:\\\\Users\\\\kec52\\\\Documents\\\\BLtext_research\\\\BritishLib\\\\0007_clean_sent.txt\"))\n",
    "ProText9 = Text(nltk.corpus.gutenberg.words(\"C:\\\\Users\\\\kec52\\\\Documents\\\\BLtext_research\\\\BritishLib\\\\0008_clean_sent.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_compare(word, gram_num, freq):\n",
    "    print('Pro results: ','/n')\n",
    "    proText1.concordance(word, width=80, lines=40)\n",
    "    print('_________________________________________________________________________________')\n",
    "    print('\\n')\n",
    "    proText2.concordance(word, width=80, lines=40)\n",
    "    filter_ngrams(ProText1, word, gram_num, freq)\n",
    "    print('________________________' , '\\n')\n",
    "    filter_ngrams(ProText2, word, gram_num , freq)\n",
    "    print('\\n')\n",
    "    print('___________________________________________________________________________________________________________')\n",
    "    print('\\n')\n",
    "    proText3.concordance(word, width=80, lines=40)\n",
    "    print('_________________________________________________________________________________')\n",
    "    print('\\n')\n",
    "    proText4.concordance(word)\n",
    "    filter_ngrams(proText3, word, gram_num, freq)\n",
    "    print('________________________' , '\\n')\n",
    "    filter_ngrams(proText4, word, gram_num , freq)\n",
    "    print('___________________________________________________________________________________________________________')\n",
    "    print('\\n')\n",
    "    proText5.concordance(word, width=80, lines=40)\n",
    "    print('_________________________________________________________________________________')\n",
    "    print('\\n')\n",
    "    proText5.concordance(word)\n",
    "    filter_ngrams(proText5, word, gram_num, freq)\n",
    "    print('________________________' , '\\n')\n",
    "    filter_ngrams(proText5, word, gram_num , freq)\n",
    "    print('___________________________________________________________________________________________________________')\n",
    "    print('\\n')\n",
    "    proText6.concordance(word, width=80, lines=40)\n",
    "    print('_________________________________________________________________________________')\n",
    "    print('\\n')\n",
    "    proText6.concordance(word)\n",
    "    filter_ngrams(proText6, word, gram_num, freq)\n",
    "    print('________________________' , '\\n')\n",
    "    filter_ngrams(proText6, word, gram_num , freq)\n",
    "    print('___________________________________________________________________________________________________________')\n",
    "    print('\\n')\n",
    "    proText7.concordance(word, width=80, lines=40)\n",
    "    print('_________________________________________________________________________________')\n",
    "    print('\\n')\n",
    "    proText7.concordance(word)\n",
    "    filter_ngrams(proText7, word, gram_num, freq)\n",
    "    print('________________________' , '\\n')\n",
    "    filter_ngrams(proText7, word, gram_num , freq)\n",
    "    print('___________________________________________________________________________________________________________')\n",
    "    print('\\n')\n",
    "    proText8.concordance(word, width=80, lines=40)\n",
    "    print('_________________________________________________________________________________')\n",
    "    print('\\n')\n",
    "    proText8.concordance(word)\n",
    "    filter_ngrams(proText8, word, gram_num, freq)\n",
    "    print('________________________' , '\\n')\n",
    "    filter_ngrams(proText8, word, gram_num , freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_compare('vaccination',3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
