{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports needed and logging\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from time import time\n",
    "import pickle\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import gensim \n",
    "from gensim import corpora, models, similarities, utils\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import MmCorpus, Dictionary\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import pyLDAvis.gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.text import Text\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "logging.basicConfig(format= '%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lemmatize for verbs\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        lemma = word\n",
    "    return lemma\n",
    "\n",
    "#lemmatize for nouns\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean texts\n",
    "def clean_txt(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    #tokens = [get_lemma(token) for token in tokens]\n",
    "    #tokens = [get_lemma2(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iter_documents(top_directory):\n",
    "    \"\"\"Iterate over all documents, yielding a document (=list of utf8 tokens) at a time.\"\"\"\n",
    "    for root, dirs, files in os.walk(top_directory):\n",
    "        for file in filter(lambda file: file.endswith('.txt'), files):\n",
    "            document = io.open(os.path.join(root, file), encoding='utf=8', errors='ignore').read() # read the entire document, as one big string\n",
    "            x = clean_txt(document) # or whatever tokenization suits\n",
    "            yield x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    \n",
    "    def __init__(self, top_dir):\n",
    "        self.top_dir = top_dir\n",
    "        self.dictionary = gensim.corpora.Dictionary(iter_documents(top_dir))\n",
    "        self.dictionary.filter_extremes(no_below=2, keep_n=30000) # check API docs for pruning params \n",
    "        \n",
    "    def __len__(self):\n",
    "        count = 0\n",
    "        for root, dirs, files in os.walk(self.top_dir):\n",
    "            for file in filter(lambda file: file.endswith('.txt'), files):\n",
    "                count += 1\n",
    "        self._data_len = int(count)\n",
    "        return self._data_len\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        for tokens in iter_documents(self.top_dir):\n",
    "            yield self.dictionary.doc2bow(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#returns corpus, serialized corpus, and dict. args: top_dir, corpus_name\n",
    "#saves serialized corpus mm and dictionary as .dict\n",
    "#both args in ''\n",
    "\n",
    "def get_corpus_dict(top_dir, corpus_name):\n",
    "    corpus = MyCorpus(top_dir)\n",
    "    #save corpus\n",
    "    pickle.dump(corpus, open(corpus_name + '.pkl', 'wb'))\n",
    "    #save dictionary\n",
    "    dictionary = corpus.dictionary\n",
    "    dictionary.save(corpus_name +'_dictionary.dict')\n",
    "    new_corpus = [vector for vector in iter(corpus)]\n",
    "    corpora.MmCorpus.serialize(corpus_name+'_serialized.mm', new_corpus)\n",
    "    # Building reverse index.\n",
    "    for (token, uid) in dictionary.token2id.items():\n",
    "        dictionary.id2token[uid] = token\n",
    "    return corpus, new_corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_model(corpus,dictionary, num_passes, num_topics):\n",
    "    start = time()\n",
    "    LDA = gensim.models.ldamodel.LdaModel(corpus, id2word = dictionary, passes = num_passes, num_topics = num_topics)\n",
    "    #print time\n",
    "    print ('used: {:.2f}s'.format(time()-start))\n",
    "    return LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_topics(model, num_words):\n",
    "    topics = model.print_topics(num_words=num_words)\n",
    "    for topic in topics:\n",
    "        print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print nice df of topics\n",
    "def get_topics(corpus_name, model, num_topics):\n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        words = model.show_topic(i, topn = 20)\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words]\n",
    "    topics = pd.DataFrame(word_dict)\n",
    "    print(topics)\n",
    "    topics.to_csv(corpus_name+'_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print nice df of topics\n",
    "def get_topics(corpus_name, model, num_topics):\n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        words = model.show_topic(i, topn = 20)\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words]\n",
    "    topics = pd.DataFrame(word_dict)\n",
    "    print(topics)\n",
    "    topics.to_csv(corpus_name+'_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_ldavis(model, serial_corpus, dictionary, corpus_name):\n",
    "    pyLDAvis.enable_notebook()\n",
    "    data = pyLDAvis.gensim.prepare(model, serial_corpus, dictionary)\n",
    "    pyLDAvis.save_html(data, corpus_name+'_lda.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#topic coherence - human interpretability of topic model using cv coherence\n",
    "# arguments: dictionary = Gensim dictionary, corpus =  Gensim corpus, limit = max num topics\n",
    "# Returns: lm_list = List of LDA topic models and c_v  = Coherence values corresponding to LDA model with respective num_topics\n",
    "\n",
    "def evaluate_graph(dictionary, corpus, limit):\n",
    "    c_v = []\n",
    "    lm_list = []\n",
    "    for num_topics in range(1, limit):\n",
    "        lm = lda_model(corpus, dictionary,30, num_topics)\n",
    "        lm_list.append(lm)\n",
    "        cm = CoherenceModel(model=lm, corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
    "        c_v.append(cm.get_coherence())\n",
    "        \n",
    "    # Show graph\n",
    "    x = range(1, limit)\n",
    "    plt.plot(x, c_v)\n",
    "    plt.xlabel(\"num_topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"c_v\"), loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    return lm_list, c_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
